{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyurashku/IST_718_Big_Data_Analytics_Jane_Street_Kaggle_Competition/blob/main/ist_718_final_project_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.oauth2 import service_account  # Import this\n",
        "\n",
        "TOKEN_INFO = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"smiling-height-454617-m1\",\n",
        "  \"private_key_id\": \"897109ca7721a816d15a923f347c2924d93de443\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQC7T65LgZthhFsp\\n4IUBE/N2Cgs6FNG0hEjiMEJbHtIoh67OJMMvGJRMa3RDtrsOfFGrB9p2N0jUPOqR\\ns+X8ABdqiknLyZ6DdeIh2rNzwSXnM20RTyjP3scQDjVzkWtqUFeAmIyvtpSmTTbq\\nhc+5rFbktFlE1aB/UEyU3neS1PcDZrVRPQNqbXCdeovlsKuEF2GtVKX7w8wXtS3D\\nTx81FpWRGzHr7CsLeYImB3YB+lmlosfQV9dW9aIzt234onXNmochP9P4mRrp9ecG\\n0u+G3fyLH76VCqvGubRviGN7Ngqmz3X3bjNaVC1IoPAmHe5/HD6nfeg384pQtQFt\\nT+KFmxJ5AgMBAAECggEAAi8+DNAN0YerTy6407gO2Svpe5uYjS6/HDGMjzwpyU05\\nIwWsvRTf+JAxl+8+o+mb7jm2OOC7hGDS1NuF7XgheHL1GgpXmJg8XewLYb3j75wF\\nmUkUucU9tG3l/ltjfFGZjlIOefzQI6xiMpQC8IXBBwPPcre8b8pY9PeElNLPsAYd\\nL2iUNdh12vujF1nqNLTW3pz6XUtZB25lOkRyN4elSbYznRZrD7C/1U6NC0lae3AI\\nRECNtwN3pCHPaVoKhMzJtEK3Ce/xudBnJdd2iRqze5T+QXtlAaoJMUu93xneWye/\\nNKxM6Ssc2XhubN+3wsqtVtndBosHh6kChVdWRjSEPQKBgQDjQPubFyhaXhJg+LEY\\nk1pAFQfFsqIGOby53h0UdUpjkZfEXtxdf1COeEhsQjCYJ/YiqsyBHXPXQeWck2Hw\\napBIlRcr+dmbzI+PzXX5e59X9Q8WACy3qGi60qXvpWKdxQqA0J4I1onD+VC4fKnt\\n4buTBytXW1BXuw05b8CGBSwEmwKBgQDTAUM1uE50GoLTZpOmmt6LDwdw4qCan4fr\\nBVi7YNWiamqIsgFZ2QjYpFnh8ldl5qu9RzWa60uJp8KQXUkMpCMyBvBJjIqwp3Xf\\nPIIDI7ol4w1Ae05TxxuoOYsXOSohoDMkH/HpC8PuFSoR74SJUd8B+TwqrthQ1Ryc\\nqqfwsfZUewKBgQCYNzmQSzWbOfGDhHBSrrg/vhZjpGAEXX383TqLLnjyMl3+iS9/\\nbLyPgR+5a6hS8klaNR8fyJOD9j6O3uU1/JrCEyLfMM5CHgV2478GqXg0mQ9OxDUv\\nmxzzLg85coSYeperapDACLtkScV4nP/RN2bFd4LfU2mAKekUtLa2zhYKywKBgQDC\\n626n6Hto27ixCxgolvYv5ZFNIb2VOPv5CrZeWmTfpYiGgFGpK0n1v1Vc2V8NPlq1\\n/lsxolzka/nbHKHHsLTmmOxONStFZ10G/MFpnw5tw2JRh9bio5MUAxxPNrqJ15CG\\ntY/mZbz4acq0SNvZcq0dCJ0hHOWtPkExJKEBQ5S3kwKBgF81x8n6e94DOkWFIISe\\n9rgvOsXcOaxeJyBRo8e0mIbD1/weh6ZWm10e3sixsMBvGAC1ozO0N68aU4zTWVJX\\nPbF/dpkTiTPUUKENhuvGxWS0eZOOYvTKa6Re+QD+MJ34TwgmUmGWHEDCrtIgT71S\\nQnCLZx8UAcbPulFBtGrI/aIO\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"service-ist718@smiling-height-454617-m1.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"110832650342924718156\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/service-ist718%40smiling-height-454617-m1.iam.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}\n",
        "def initialize_drive():\n",
        "    \"\"\"Initialize the Google Drive service using embedded credentials.\"\"\"\n",
        "    credentials = service_account.Credentials.from_service_account_info(\n",
        "        TOKEN_INFO,\n",
        "        scopes=['https://www.googleapis.com/auth/drive']\n",
        "    )\n",
        "\n",
        "    service = build('drive', 'v3', credentials=credentials)\n",
        "    return service\n",
        "\n",
        "def list_folder_files(service, folder_id):\n",
        "    \"\"\"Recursively list all files in a folder and its subfolders.\"\"\"\n",
        "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
        "    results = service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, mimeType, parents)\"\n",
        "    ).execute()\n",
        "\n",
        "    files = results.get('files', [])\n",
        "\n",
        "    subfolders = [f for f in files if f['mimeType'] == 'application/vnd.google-apps.folder']\n",
        "    all_files = [f for f in files if f['mimeType'] != 'application/vnd.google-apps.folder']\n",
        "\n",
        "    for folder in subfolders:\n",
        "        print(f\"Entering subfolder: {folder['name']}\")\n",
        "        all_files.extend(list_folder_files(service, folder['id']))\n",
        "\n",
        "    return all_files\n",
        "\n",
        "def download_file(service, file_id, file_name):\n",
        "    \"\"\"Download a file in chunks to avoid memory overload.\"\"\"\n",
        "    request = service.files().get_media(fileId=file_id)\n",
        "\n",
        "    file_size = int(service.files().get(fileId=file_id, fields=\"size\").execute().get(\"size\", 0))\n",
        "    print(f\"Downloading {file_name} ({file_size / (1024*1024):.2f} MB)...\")\n",
        "\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        downloader = MediaIoBaseDownload(f, request, chunksize=50 * 1024 * 1024)  # 50MB chunks\n",
        "        done = False\n",
        "        while not done:\n",
        "            try:\n",
        "                status, done = downloader.next_chunk()\n",
        "                if status:\n",
        "                    print(f\"Progress: {int(status.progress() * 100)}%\")\n",
        "            except Exception as e:\n",
        "                print(f\"Download failed for {file_name}: {e}\")\n",
        "                return False\n",
        "\n",
        "    print(f\"Successfully downloaded: {file_name}\")\n",
        "    return True\n",
        "\n",
        "def download_all_files(service, folder_id):\n",
        "    \"\"\"Download all files from a folder and its subfolders.\"\"\"\n",
        "    files = list_folder_files(service, folder_id)\n",
        "\n",
        "    if not files:\n",
        "        print(\"No files found in the folder.\")\n",
        "        return\n",
        "\n",
        "    for file in files:\n",
        "        print(f\"Starting download of: {file['name']}\")\n",
        "        if download_file(service, file['id'], file['name']):\n",
        "            print(f\"Downloaded: {file['name']}\")\n",
        "        else:\n",
        "            print(f\"Skipping {file['name']} due to download error.\")\n",
        "\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    drive_service = initialize_drive()\n",
        "    folder_id = \"1ux1YSviijxI5o0j1JabkiPh2ZIeiDKZ1\"\n",
        "    download_all_files(drive_service, folder_id)\n",
        "    #https://drive.google.com/drive/folders/1ux1YSviijxI5o0j1JabkiPh2ZIeiDKZ1?usp=drive_link"
      ],
      "metadata": {
        "id": "zqudlq9jNw-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/part-0.parquet\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('ParquetLoader').getOrCreate()\n",
        "\n",
        "folder_path = \"/content\"\n",
        "\n",
        "DF0 = spark.read.parquet(f'{folder_path}/part-0.parquet').limit(20000)\n",
        "DF1 = spark.read.parquet(f'{folder_path}/part-1.parquet').limit(20000)\n",
        "DF2 = spark.read.parquet(f'{folder_path}/part-2.parquet').limit(20000)\n",
        "DF3 = spark.read.parquet(f'{folder_path}/part-3.parquet').limit(20000)\n",
        "DF4 = spark.read.parquet(f'{folder_path}/part-4.parquet').limit(20000)\n",
        "DF5 = spark.read.parquet(f'{folder_path}/part-5.parquet').limit(20000)\n",
        "DF6 = spark.read.parquet(f'{folder_path}/part-6.parquet').limit(20000)\n",
        "DF7 = spark.read.parquet(f'{folder_path}/part-7.parquet').limit(20000)\n",
        "DF8 = spark.read.parquet(f'{folder_path}/part-8.parquet').limit(20000)\n",
        "DF9 = spark.read.parquet(f'{folder_path}/part-9.parquet').limit(20000)\n",
        "\n",
        "combined_df = DF0.union(DF1).union(DF2).union(DF3).union(DF4) \\\n",
        "                 .union(DF5).union(DF6).union(DF7).union(DF8).union(DF9)\n",
        "\n",
        "print(f'Combined DataFrame row count: {combined_df.count()}')\n",
        "combined_df.show(30)"
      ],
      "metadata": {
        "id": "z_1emQa4N631"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}