{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet', '/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet']\n",
      "Eager DataFrame shape: (47127338, 92)\n",
      "Sample shape (frac=0.6): (28276402, 92)\n",
      "After imputation shape: (28276402, 92)\n",
      "Train shape: (23836043, 92)\n",
      "Val shape:   (4440359, 92)\n",
      "X_train shape: (23836043, 80)\n",
      "y_train shape: (23836043,)\n",
      "X_val shape:   (4440359, 80)\n",
      "y_val shape:   (4440359,)\n",
      "R2 on 60% sample + mean imputation: 0.0079\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1) Gather all Parquet files\n",
    "all_files = glob.glob(\n",
    "    \"/Users/nicky/Documents/codebase/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=*/part-0.parquet\"\n",
    ")\n",
    "print(\"Files found:\", all_files)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Set your fraction for sampling. We'll pick 0.60 (60%).\n",
    "SAMPLE_FRAC = 0.60\n",
    "\n",
    "# Decide how you want to fill nulls. Options: \"mean\" or \"median\"\n",
    "FILL_STRATEGY = \"mean\"  # or \"median\"\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 2) Create a list of lazy frames and concatenate them\n",
    "df_list = [pl.scan_parquet(f) for f in all_files]\n",
    "df_lazy = pl.concat(df_list)  # still lazy\n",
    "\n",
    "# 3) Collect to get an eager DataFrame\n",
    "df_eager = df_lazy.collect()\n",
    "print(\"Eager DataFrame shape:\", df_eager.shape)\n",
    "\n",
    "# 4) Sample 60% of rows (if you want strictly 60%, you can do random sampling)\n",
    "#    But if you must preserve the time order, you could do a direct slice.\n",
    "#    For random sampling:\n",
    "n_to_sample = math.floor(df_eager.height * SAMPLE_FRAC)\n",
    "df_sample = df_eager.sample(n=n_to_sample, with_replacement=False)\n",
    "print(f\"Sample shape (frac={SAMPLE_FRAC}):\", df_sample.shape)\n",
    "\n",
    "# 5) Fill Nulls with Mean or Median for numeric columns\n",
    "#    We'll define which columns are numeric.\n",
    "numeric_cols = []\n",
    "for col, dtype in df_sample.schema.items():\n",
    "    if dtype in (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64):\n",
    "        numeric_cols.append(col)\n",
    "\n",
    "# Fill nulls column by column using Polars expressions\n",
    "if FILL_STRATEGY == \"mean\":\n",
    "    df_imputed = df_sample.with_columns([\n",
    "        pl.col(col).fill_null(strategy=\"mean\").alias(col)\n",
    "        for col in numeric_cols\n",
    "    ])\n",
    "elif FILL_STRATEGY == \"median\":\n",
    "    df_imputed = df_sample.with_columns([\n",
    "        pl.col(col).fill_null(strategy=\"median\").alias(col)\n",
    "        for col in numeric_cols\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(\"FILL_STRATEGY must be 'mean' or 'median'\")\n",
    "\n",
    "print(\"After imputation shape:\", df_imputed.shape)\n",
    "\n",
    "# 6) Time-based split (date_id cutoff = 1500)\n",
    "date_cutoff = 1500\n",
    "train_df = df_imputed.filter(pl.col(\"date_id\") < date_cutoff)\n",
    "val_df   = df_imputed.filter(pl.col(\"date_id\") >= date_cutoff)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val shape:  \", val_df.shape)\n",
    "\n",
    "# 7) Exclude non-feature columns, define target\n",
    "exclude_cols = [\n",
    "    \"date_id\", \"time_id\", \"symbol_id\",\n",
    "    \"responder_0\",\"responder_1\",\"responder_2\",\n",
    "    \"responder_3\",\"responder_4\",\"responder_5\",\n",
    "    \"responder_6\",\"responder_7\",\"responder_8\"\n",
    "]\n",
    "all_cols = df_imputed.columns\n",
    "feature_cols = [c for c in all_cols if c not in exclude_cols]\n",
    "target_col = \"responder_6\"\n",
    "\n",
    "# 8) Convert to NumPy\n",
    "X_train = train_df.select(feature_cols).to_numpy()\n",
    "y_train = train_df.select(target_col).to_numpy().ravel()\n",
    "\n",
    "X_val = val_df.select(feature_cols).to_numpy()\n",
    "y_val = val_df.select(target_col).to_numpy().ravel()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:  \", X_val.shape)\n",
    "print(\"y_val shape:  \", y_val.shape)\n",
    "\n",
    "# 9) Train an XGBoost model\n",
    "reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",  # more memory-friendly than \"auto\"\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 10) Predict and evaluate\n",
    "y_val_pred = reg.predict(X_val)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"R2 on {int(SAMPLE_FRAC*100)}% sample + {FILL_STRATEGY} imputation: {val_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codebase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
